# create a list of databases of interest, e.g.
ls ../*/*.db > visit_db_list.txt

# create symlinks in this directory for the dbs
# use command line arg to pass a different db list file
./gen_db_symlinks.sh

# create the ss_script.sh file for each db
# also add the relevant lines from ss_script_header.txt required to run on cuillin
# 1st cmd line arg: population to run ss_metrics on: mba_5k, l7_5k, granvik_5k, granvik_pha_5k, occ_rmax5_5k, occ_rmax20_5k
# 2nd cmd line arg: a db keyword to select a particular set of dbs (e.g. baseline_v3_2 searches *baseline_v3_2*.db)
./gen_ss_script.sh

# put all ss_script.sh files into a slurm script for batch submission
# the default info in slurm_header.txt assumes 1 job per core
# 1: use command line arg to select memory per task, e.g. 2GB
# 2: select the dynamical population, e.g. mba_5k
# 3: select a db keyword to run on selected dbs, e.g. baseline_v3_2
./gen_slurm.sh

# see also gen_slurm_pop.sh to generate runs for all dynamical populations in a particular baseline

# the resulting file test.slurm must be renamed and SBATCH details filled out:
# jobname
# ntasks
# total memory
# run time
# some of the tasks might need to be deleted to fit within the desired ntasks

# then submit the .slurm file
sbatch .slurm

# retrieve results from cuillin:
rsync -nav -e ssh cuillin:/home/jrobinson/rubin_templates/ss_metrics/*baseline_v3_3*noTwi* . --exclude="*.npz" --exclude="*.png" --exclude="*.pdf" --exclude="*.out" --exclude="*baseline*.db" --exclude="*.slurm" --exclude="*.sh" --exclude="*.txt"

# see notebook at: http://localhost:8888/notebooks/Documents/proposals%20presentations%20projects/LSST_PCW_2023/results/read_metric_dbs.ipynb

# Dynamical populations for moving object metrics:
# https://github.com/lsst-sssc/SSSC_test_populations_gitlfs/tree/main/MAF_TEST
